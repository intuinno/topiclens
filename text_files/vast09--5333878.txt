Evaluating visual analytics systems for investigative analysis: Deriving design principles from a case study.

Despite the growing number of systems providing visual
analytic support for investigative analysis, few empirical studies of
the potential benefits of such systems have been conducted, particularly
controlled, comparative evaluations. Determining how such systems foster
insight and sensemaking is important for their continued growth and
study, however. Furthermore, studies that identify how people use such
systems and why they benefit (or not) can help inform the design of new
systems in this area. We conducted an evaluation of the visual analytics
system Jigsaw employed in a small investigative sensemaking exercise,
and we compared its use to three other more traditional methods of
analysis. Sixteen participants performed a simulated intelligence
analysis task under one of the four conditions. Experimental results
suggest that Jigsaw assisted participants to analyze the data and
identify an embedded threat. We describe different analysis strategies
used by study participants and how computational support (or the lack
thereof) influenced the strategies. We then illustrate several
characteristics of the sensemaking process identified in the study and
provide design implications for investigative analysis tools based
thereon. We conclude with recommendations for metrics and techniques for
evaluating other visual analytics investigative analysis tools.